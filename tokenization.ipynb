{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and detokenization are important parts of an LLM pipeline. This notebook was to explore those steps.\n",
    "\n",
    "For context, an LLM pipeline takes a bunch of text input and predicts the next word. It works roughly like this:\n",
    "1. **Tokenize**; breaks the text input into tokens. The tokenization algorithm is independant of how LLMs are trained. Tokens are often sub-words that have meaning. For example, \"dogs\" might become \"dog\" and \"s\".\n",
    "2. **Embed**; each token is converted into a high-dimensional vector called an embedding. Embeddings are generated by a layer in the LLM called the embedding layer that is trained in parallel with the rest of the LLM, taking advantage of the transformer-based architecture down-stream that provides context awareness for each token so that a given token (like \"wind\") can have multiple embeddings depending on it's context (e.g., if it is used as a verb or noun).\n",
    "3. **Predict embedding**; based on preceding tokens' embeddings, the transformer-based neural network predicts the embedding for the next token.\n",
    "4. **Predict token**; a token is selected based on the predicted embedding, typically sampled from a nearest-neighbor probability distribution in order to create diversity in the generated outputs.\n",
    "5. **Predict word**; the next word is selected based on the predicted token(s). This is not 1-1; as the tokens for \"dog\" and \"s\" may be output as a single word \"dogs\".\n",
    "\n",
    "This is repeated, with the predicted word now being appended to the input, until an \"end\" token is output.\n",
    "\n",
    "For more information about the architecture and training of transformers (steps 2-4), see the <a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a> paper, or consider taking the <a href=\"https://www.coursera.org/learn/generative-ai-with-llms/home/welcome\">Generative AI with Large Language Models</a> Coursera class.\n",
    "\n",
    "This notebook is for exploring the tokenization/detokenization steps (1 and 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization/detokenization using various tokenizers from huggingface/transformers\n",
    "#!pip install transformers\n",
    "\n",
    "# Import and initialize a few tokenizers\n",
    "from transformers import (\n",
    "    BertTokenizer, GPT2Tokenizer, RobertaTokenizer\n",
    ")\n",
    "tokenizers = {\n",
    "    'BERT': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "    'GPT-2': GPT2Tokenizer.from_pretrained('gpt2'),\n",
    "    'RoBERTa': RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "}\n",
    "\n",
    "print (\"Tokenizers from hugging face\")\n",
    "\n",
    "# Input text\n",
    "text = \"Huh, the cat's toy is\"\n",
    "print (\"Input Text:\", text)\n",
    "\n",
    "# Tokenization\n",
    "for tokenizer_name, tokenizer in tokenizers.items():\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print (tokenizer_name)\n",
    "    print(\"\\tTokens:\", tokens)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(\"\\tToken IDs:\", token_ids)\n",
    "    decoded_text = tokenizer.decode(token_ids)\n",
    "    print(\"\\tDecoded Text:\", decoded_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple hand-crafted tokenization\n",
    "\n",
    "print (\"Simple hand-crafted tokenizers\")\n",
    "\n",
    "# Define a simple tokenizer\n",
    "def simple_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Define a more sophisticated tokenizer\n",
    "import re\n",
    "def more_complex_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "\n",
    "tokenizers = {\n",
    "    'Words': simple_tokenizer,\n",
    "    'Split off possessives': more_complex_tokenizer,\n",
    "}\n",
    "\n",
    "# Input text\n",
    "text = \"Huh, the cat's toy is\"\n",
    "print (\"Input Text:\", text)\n",
    "\n",
    "for tokenizer_name, tokenizer in tokenizers.items():\n",
    "    tokens = tokenizer(text)\n",
    "    print (tokenizer_name)\n",
    "    print(\"\\tTokens:\", tokens)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
